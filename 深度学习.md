# 深度学习

## 线性回归

### SoftMax回归

![1715740557255](.\typora-user-images\1715740557255.png)

与线性回归一样，`softmax`回归也是一个单层神经网络。由于计算每个输出$0_1$、$0_2$和$0_3$`取决于所有输入`$x_1、x_2、x_3和x_4$`,所以`softmax回归的输出层也是全连接层。

我们希望模型的输出$y_j$可以视为属于类j的概率，例如，如果$\hat{y}_1$、$\hat{y}_2$和$\hat{y}_3$分别为0.1、0.8和0.1，那么我们预测的类别是2，在我们的例子中代表"鸡"。

**`softmax`函数能够将未规范化的预测变换为非负数并且总和为1**，同时让模型保持可导的性质。为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负。为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。如下式：
$$
y^i=softmax(o)其中 y^j =\frac{exp(o_j)}{ \sum{}{}_kexp(o_k)}
$$
对于所有的j总有$0≤y^j ≤1$。因此，$y^j$可以视为一个正确的概率分布。尽管`softmax`是一个非线性函数，但`softmax`回归的输出仍然由输入特征的仿射变换决定。因此，`softmax`回归是一个线性模型

#### 解析解

与其他大部分模型不同，线性回归的解可以用一个公式简单地表达出来，这类解叫作解析解（analytical solution）

### 损失函数

损失函数（loss function）能够量化目标的实际值与预测值之间的差距。通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。
$$
l(y,y^i)=-\sum_{j=1}^{q}y_ilog\hat{y}_i
$$

上面的损失函数通常被称为交叉熵损失，它用于衡量实际输出分布与期望输出分布(或者说是预测分布与真实分布)之间的差异。在具体的形式上，交叉熵损失函数通常用来计算分类问题中的二元分类或多分类问题中的损失。

### 随机梯度下降

梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值）关于模型参数的导数（在这里也可
以称为梯度）。但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本，这种变体叫做小批量随机梯度下降（minibatch stochastic gradient descent）

## 多层感知机

我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制，**使其能处理更普遍的函数关系类型**。要做到这一点，最简单的方法是将许多全连接层堆叠在一起。每一层都输出到上面的层，直到生成最后的输出。我们可以把前L−1层看作表示，把最后一层看作线性预测器。这种架构通常称为多层感知机`(multilayer perceptron)`，通常缩写为`MLP`。

![1715743918372](.\typora-user-images\1715743918372.png)

 上图是一个单隐藏层的多层感知机，具有5个隐藏单元。输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算。因此，这个多层感知机中的层数为2。注意，这两个层都是全连接的。每个输入都会影响隐藏层中的每个神经元，而隐藏层中的每个神经元又会影响输出层中的每个神经元。

我们按如下方式计算单隐藏层多层感知机的输出$O∈R^{n×q}$：
$$
H=XW^{(1)} +b^{(1)}\\
O=HW^{(2)}+b^{(2)}
$$
$H ∈ R^{n×h}$表示隐藏层的输出,$W^{(1)} ∈R^{d×h}$表示隐藏层权重，$b^{(1)} ∈R^{1×h}$表示隐藏层偏置以及输出层权重$W^{(2)} ∈R^{d×h}$ 和输出层偏置$b^{(2)} ∈R^{1×h}$

对于任意权重值，我们只需合并隐藏层，便可产生具有参数$W = W^{(1)}W^{(2)}$和$b=b^{(1)}W^{(2)} +b^{(2)}$ 的等价单层模型：
$$
O=(XW^{(1)} +b^{(1)})W^{(2)} +b^{(2)} = XW^{(1)}W^{(2)} +b^{(1)}W^{(2)} +b^{(2)} = XW+b.
$$
为了让多层感知机和线性模型进行区分，我们还需要一个激活函数

$$
H=σ(XW^{(1)} +b^{(1)})\\
O=HW^{(2)}+b^{(2)}
$$
为了构建更通用的多层感知机，我们可以继续堆叠这样的隐藏层，例如$H^{(1)}=σ(XW^{(1)} +b^{(1)})$和$H^{(2)}=
σ2(H^{(1)}W^{(2)} +b^{(2)})$，一层叠一层，从而产生更有表达能力的模型。

### 激活函数

激活函数通过计算加权和并加上偏置来确定神经元是否应该被激活，它们将输入信号转换为输出的可微运算。大多数激活函数都是非线性的。

#### ReLU函数

`ReLU`提供了一种非常简单的非线性变换。给定元素x，`ReLU`函数被定义为该元素与0的最大值：
$$
ReLU(x) = max(x,0).
$$
![1716702105477](.\typora-user-images\1716702105477.png)

通俗地说，`ReLU`函数通过将相应的活性值设为0，仅保留正元素并丢弃所有负元素。使用`ReLU`的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。这使得优化表现得更好，并且`ReLU`减轻了困扰以往神经网络的梯度消失问题

#### sigmoid函数

对于一个定义域在R中的输入，sigmoid函数将输入变换为区间(0,1)上的输出。
$$
sigmoid(x) =\frac{1}{1 +exp(−x)}
$$
![1716702139071](.\typora-user-images\1716702139071.png)

sigmoid在隐藏层中已经较少使用，它在大部分时候被更简单、更容易训练的`ReLU`所取代。在循环神经网络中，我们将描述利用sigmoid单元来控制时序信息流的架构。

#### tanh函数

与sigmoid函数类似，`tanh`(双曲正切)函数也能将其输入压缩转换到区间(‐1,1)上

$$
tanh(x) =\frac{1−exp(−2x)}{1+exp(−2x)}
$$
![1716702158996](.\typora-user-images\1716702158996.png)

注意，当输入在0附近时，`tanh`函数接近线性变换。函数的形状类似于sigmoid函数，不同的是`tanh`函数关于坐标系原点中心对称。

多层感知机在输出层和输入层之间增加一个或多个全连接隐藏层，并通过激活函数转换隐藏层的输出。

## 模型训练

### 欠拟合和过拟合

将模型在训练数据上拟合的比在潜在分布中更接近的现象称为过拟合(over fitting)，用于对抗过拟合的技术
称为正则化(regularization)

**训练误差**是指，模型在**训练数据集**上计算得到的误差。

**泛化误差**是指，模型应用在同样从原始样本的分布中抽取的无限多数据样本时，**模型误差的期望**

K折交叉验证
当训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个合适的验证集。这个问题的一个流行的解决方案是采用K折交叉验证。这里，原始训练数据被分成K个不重叠的子集。然后执行K次模型训练和验证，每次在K−1个子集上进行训练，并在剩余的一个子集(在该轮中没有用于训练的子集)上进行验证。最后，通过对K次实验的结果取平均来估计训练和验证误差。

![1715747643789](.\typora-user-images\1715747643789.png)

如果没有足够的数据，简单的模型可能更有用。对于许多任务，深度学习只有在有数千个训练样本时才优于线性模型。从一定程度上来说，深度学习目前的生机要归功于廉价存储、互联设备以及数字化经济带来的海量数据集。

### 权重衰减

**权重衰减**（Weight Decay）是一种正则化技术，用于防止神经网络在训练过程中过拟合。它通过对模型的权重施加一个额外的惩罚项，使得模型的权重不会过大，从而达到防止过拟合的效果。

权重衰减在梯度下降过程中被引入到损失函数中，通过在损失函数中添加权重的平方和项来控制模型的复杂度。通常，这个额外的惩罚项被称为**L2正则化**。

权重衰减的目的是使权重值趋于较小，从而避免模型变得过于复杂。较小的权重值通常对应较简单的模型，简单的模型通常具有更好的泛化能力（即在未见过的数据上表现更好）。
$$
Loss(W)=L(w,b) + \frac{λ}{2}||w||^2
$$
这里，$||w||^2$表示权重 W的 L2 范数（即权重的平方和）,而 λ是一个正则化系数，表示控制权重衰减的强度。如果λ<0的话，后面的一项为0，如果λ>0的话可以限制$∥w∥^2$的大小，当我们取一个二次函数的导数时，2和$ \frac{1}{2}$会抵消

L2正则化回归的小批量随机梯度下降更新如下式

$$
w←(1−ηλ)w− \frac{η}{|B|}\sum_{i∈B} x(i)( w^⊤x^{(i)} +b−y^{(i)}) .
$$
根据估计值与观测值之间的差异来更新w,然而，我们同时也在试图将w的大小缩小到零。这就是为什么这种方法有时被称为**权重衰减。**我们仅考虑惩罚项，优化算法在训练的每一步衰减权重。与特征选择相比，权重衰减为我们提供了一种连续的机制来调整函数的复杂度。较小的λ值对应较少约束的w，而较大的λ值对w的约束更大

#### 权重衰减的作用机制

在每次梯度更新时，除了根据损失函数对权重进行调整外，权重衰减项会对权重进行额外的调整。这个调整会将权重逐渐缩小到接近于零，从而防止权重的无限增大。这种"衰减"使得模型对每个输入变量的依赖性降低，进而提升模型的泛化能力。

- **防止过拟合**：权重衰减有效限制了模型的复杂性，防止模型记住训练数据中的噪音或不相关的特征，从而提高模型在测试集上的表现。
- **平滑模型**：通过缩小权重值，模型更倾向于学习到较平滑的、一般化的模式，这意味着在遇到未见过的数据时模型表现会更加鲁棒。

### 暂退法(Dropout)

**Dropout** 是深度学习中一种非常流行的正则化技术，用于防止模型过拟合。它的核心思想是在每次训练时，随机丢弃部分神经元，使得网络无法过度依赖某些特定的神经元，从而迫使模型更加鲁棒、提高泛化能力。

![1728016448958](.\typora-user-images\1728016448958.png)

一个"**好**"的预测模型，能在未知的数据上有很好的表现；

平滑性:函数不应该对其输入的微小变化敏感。例如，当我们对图像进行分类时，我们预计向像素添加一些随机噪声应该是基本无影响的。

将高斯噪声添加到线性模型的输入中。在每次训练迭代中，从均值为零的分布$ϵ∼N(0,σ^2)$ 采样噪声添加到输入x，从而产生扰动点$x′=x+ϵ$，预期是$E[x′]=x。$具体做法如下图所示

![1716101255448](.\typora-user-images\1716101255448.png)

此处h和h'应当有一样的效果。h'是在对h进行dropout之后的结果

#### **Dropout 的影响**

- **提高泛化能力**：Dropout 能显著提高神经网络的泛化能力，特别是在模型复杂、参数量大、训练数据不足的情况下。它强制网络在每次训练时学到不同的子网络结构，使得模型在推理时表现更加鲁棒。
- **防止神经元的协同适应**：当网络的某些神经元过度依赖其他神经元的输出时，模型可能会偏向学习某些特定的模式。Dropout 打破了这种协同适应，迫使网络学到更多独立和通用的特征。
- **隐式的集成学习**：Dropout 在每次迭代中相当于训练一个不同的子网络，因此整个训练过程就类似于集成了多个子模型。每个子模型学习到的特征不同，因此最终模型会有更强的泛化能力。

通常，我们在测试时不用暂退法。给定一个训练好的模型和一个新的样本，我们不会丢弃任何节点，因此不需要标准化。然而也有一些例外：一些研究人员在测试时使用暂退法，用于估计神经网络预测的"不确定性"：如果通过许多不同的暂退法遮盖后得到的预测结果都是一致的，那么我们可以说网络发挥更稳定。

#### **Dropout 率的选择**

Dropout 的丢弃率 ppp 是一个需要调节的超参数。通常情况下，丢弃的比例为 0.2 到 0.5 是一个较为常见的选择。

- **浅层网络**：如果网络较浅，建议使用较小的 Dropout 比率（例如 p=0.2p = 0.2p=0.2 到 0.30.30.3）。
- **深层网络**：如果网络较深，建议使用较高的 Dropout 比率（例如 p=0.5p = 0.5p=0.5），因为深层网络有更多的冗余神经元。

需要注意的是，Dropout 的丢弃率设置得过大可能会导致欠拟合，因为丢弃了太多神经元，模型无法有效学习数据的特征。

### 前向、反向传播和计算图

- 前向传播是指将输入数据通过神经网络的各层传递，计算出最终的输出结果（预测值）。在此过程中，每一层的神经元从上一层接收输入，计算加权和并应用激活函数，直到输出层得到最终结果。
- 反向传播是用于更新神经网络参数（权重和偏置）的算法。它通过计算损失函数相对于各层权重的偏导数（梯度），然后使用优化算法（如梯度下降）根据这些梯度更新权重，从而使损失函数最小化。
- 计算图是用于表示神经网络中计算过程的有向无环图。在计算图中，每一个节点代表一个操作（例如加法、乘法、激活函数等），边代表操作之间的依赖关系。计算图直观地描述了前向传播、反向传播过程中不同变量之间的关系。

#### 三者之间的关系

在训练模型时，神经网络会根据模型的结构自动构建计算图。计算图包括以下两部分：

1. **前向传播**：输入数据流经计算图的各个节点，生成输出。
2. **反向传播**：反向传播时，计算图中每个节点根据链式法则计算梯度，从输出端逐步向输入端传播。

#### 计算图的作用：

- **前向传播和反向传播的依赖关系**：计算图清晰地表示了不同变量和操作之间的依赖关系，使得前向传播和反向传播过程更加明确和高效。
- **自动微分**：现代深度学习框架通过构建计算图自动计算梯度。这种机制被称为**自动微分**，它能自动为每个操作生成梯度计算的代码，使反向传播过程高效和易于实现。

#### 梯度爆炸和梯度消失

- 梯度爆炸:参数更新过大，破坏了模型的稳定收敛；
- 梯度消失:参数更新过小,在每次更新时几乎不会移动,导致模型无法学习

### 学习率

学习率是机器学习和深度学习中的一个关键超参数，它决定了模型在每次参数更新时调整权重的步长大小。他对模型的影响主要包括以下两个方面：

- **控制参数更新的步长**

  学习率直接影响每次迭代时模型参数（如神经网络的权重和偏置）的更新幅度。优化算法（如梯度下降）通过计算损失函数相对于模型参数的梯度来更新参数，而学习率决定了沿着梯度方向更新的步长大小。

  - **学习率过大**：如果学习率设置得太大，模型参数的更新步长过大，可能会导致模型在训练过程中"跳跃"过最优解。这种情况会导致模型在训练过程中无法收敛，甚至在损失函数曲线中表现为震荡或发散。
  - **学习率过小**：如果学习率设置得太小，模型参数的更新步长过小，训练过程会变得非常缓慢，可能需要更多的迭代才能接近最优解。另外，过小的学习率也有可能让模型陷入局部最优解，无法探索到更好的解。

- 影响模型的收敛速度

  学习率决定了模型的收敛速度，即模型达到损失最小值所需的迭代次数。理想情况下，适当的学习率可以在较少的迭代次数中快速达到损失函数的最小值。

  - **较大的学习率**：模型会快速接近最优解，但可能会在最优解附近发生震荡，甚至跳出最优解，导致收敛不稳定。
  - **较小的学习率**：模型会平稳接近最优解，但需要较多的迭代次数，训练时间更长。

#### 学习率与优化算法的配合

不同的优化算法（如 SGD、Adam、RMSprop 等）对学习率的敏感性不同：

- **SGD（随机梯度下降）**：对学习率非常敏感，通常需要手动调整较好的学习率。
- **Adam（自适应矩估计）**：自动调整每个参数的学习率，相对不太依赖于初始学习率的设置，但仍然需要合理的初始学习率来保证训练效果。

Adam 优化器中的学习率起始值通常设置为 `0.001`，而对 SGD，可能需要使用 `0.01` 或 `0.1` 的初始学习率。

#### **学习率调度策略结合**

为了实现更好的模型性能，通常会使用学习率调度策略，动态调整学习率。常见的学习率调度策略有：

- **Step Decay**：每隔固定的步数，按一定比例缩小学习率。
- **Exponential Decay**：以指数形式衰减学习率。
- **Inverse Square Root**：根据迭代步数的平方根动态减小学习率。
- **Cosine Annealing**：在训练过程中，学习率以余弦函数形式逐渐减小。
- **Warm-up**：在训练初期，逐渐增大学习率以避免大梯度对模型的破坏，然后再减小学习率。

这些策略的目的是在训练初期使用较大的学习率以快速收敛，在训练后期使用较小的学习率进行更精细的参数调整，从而在最优解附近稳定下来。

### GPU

```cmd
nvidia-smi # 查看显卡信息
# 指定设备类型
torch.device('cpu')
torch.device('cuda')
# torch.device(f'cuda:{i}')来表示第i块GPU(i从0开始)
torch.device('cuda:1')
# 查询可用gpu的数量
torch.cuda.device_count()
```

## 卷积神经网络

卷积神经网络(convolutional neural network，CNN)是一类强大的、为处理图像数据而设计的神经网络。

对于高维感知数据，这种缺少结构的网络(MLP)可能会变得不实用。

假设我们有一个足够充分的照片数据集，数据集中是拥有标注的照片，每张照片具有百万级像素，这意味着网络的每次输入都有一百万个维度。即使将隐藏层维度降低到1000，这个全连接层也将有106×103 =109个参数。想要训练这个模型将不可实现，因为需要有大量的GPU、分布式优化训练的经验和超乎常人的耐心。

### 不变性

#### 平移不变性

平移不变性(translation invariance):不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为"平移不变性"。
$$
[H]_{i,j} = u +
\sum_{a}\sum_{b}[V]_{a,b}[X]_{i+a,j+b}
$$
我们是在使用系数`[V]a,b`对位置(i,j)附近的像素(i+a,j+b)进行加权得到`[H]i,j`。
注意:[V]a,b`的系数比`[V]i,j,a,b`少很多，因为前者不再依赖于图像中的位置。

#### 局部性

局部性(locality):神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是"局部性"原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。

如上所述，为了收集用来训练参数`[H]i,j`的相关信息，我们不应偏离到距(i,j)很远的地方。这意味着在|a| > ∆或|b| > ∆的范围之外，我们可以设置`[V]a,b = 0`。因此，我们可以将`[H]i,j`重写为
$$
[H]_{i,j} = u +
\sum_{a=-∆}^{∆}\sum_{b=-∆}^{∆}[V]_{a,b}[X]_{i+a,j+b}
$$
如上所述是一个卷积层，而卷积神经网络是包含卷积层的一类特殊的神经网络。

V被称为卷积核或者滤波器，亦或简单地称之为该卷积层的权重，通常该权重是可学习的参数。

#### 通道

图像不是二维张量，而是一个由高度、宽度和颜色组成的三维张量，比如包含1024×1024×3个像素。前两个轴与像素的空间位置有关，而第三个轴可以看作每个像素的多维表示。

因此，我们将X索引为`[X]i,j,k`。由此卷积相应地调整为`[V]a,b,c`，而不是`[V]a,b`。

为了支持输入X和隐藏表示H中的多个通道，我们可以在V中添加第四个坐标，即[V]a,b,c,d。
$$
[H]_{i,j,d} = 
\sum_{a=-∆}^{∆}\sum_{b=-∆}^{∆}\sum_{c}[V]_{a,b,c,d}[X]_{i+a,j+b,c}
$$
其中隐藏表示H中的索引d表示输出通道，而随后的输出将继续以三维张量H作为输入进入下一个卷积层。所以，可以定义具有多个通道的卷积层，而其中V是该卷积层的权重。

### 图像卷积

#### 互相关运算

在卷积层中，输入张量和核张量通过互相关运算产生输出张量。

![1716110878305](.\typora-user-images\1716110878305.png)

蓝色部分是第一个输出元素:0 ×0+1×1+3×2+4×3=19，其他位置同理，在二维互相关运算中，卷积窗口从输入张量的左上角开始，从左到右、从上到下滑动。

#### 卷积层

卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。

```python
class Conv2D(nn.Module):    
    def __init__(self, kernel_size):        			    		super().__init__()        
        self.weight = nn.Parameter(torch.rand(kernel_size))         self.bias = nn.Parameter(torch.zeros(1))    
     def forward(self, x):        
        return corr2d(x, self.weight) + self.bias
```

高度和宽度分别为h和w的卷积核可以被称为h×w卷积或h×w卷积核。我们也将带有h×w卷积核的卷积层称为h×w卷积层。

#### 特征映射和感受野

上图中输出的卷积层有时被称为特征映射(featuremap)，因为它可以被视为一个输入映射到下一层的空间维度的转换器。

在卷积神经网络中，对于某一层的任意元素x，其感受野(receptivefield)是指在前向传播期间可能影响x计算的所有元素(来自所有先前层)

### 填充和步幅

假设输入形状为$n_h×n_w$，卷积核形状为$k_h×k_w$，那么输出形状将是$(n_h−k_h+1)×(n_w−k_w+1)$。因此，卷积的输出形状取决于输入形状和卷积核的形状。

#### 填充

在应用多层卷积时，我们常常丢失边缘像素。由于我们通常使用小卷积核，因此对于任何单个卷积，我们可能只会丢失几个像素。但随着我们应用许多连续卷积层，累积丢失的像素数就多了。解决这个问题的简单方法即为填充

填充(padding)：在输入图像的边界填充元素(通常填充元素是0)

例如:上图中我们将3×3输入填充到5×5，那么它的输出就增加为4×4。阴影部分是第一个输出元素以及用于输出计算的输入和核张量元素：0×0+0×1+0×2+0×3=0。

![1716112812830](.\typora-user-images\1716112812830.png)

通常，如果我们添加$p_h$行填充(大约一半在顶部，一半在底部)和$p_w$列填充(左侧大约一半，右侧一半)，则输出形状将为
$$
(n_h −k_h +p_h +1)×(n_w −k_w +p_w +1)
$$


#### 步幅

## 循环神经网络

如果说卷积神经网络可以有效地处理空间信息，那么循环神经网络(recurrent neural network  RNN)则可以更好地处理序列信息。循环神经网络通过引入**状态变量**存储过去的信息和当前的输入，从而可以确定当前的输出。

### 序列模型

#### 自回归模型

假设在现实情况下相当长的序列$x_{t−1},...,x_1$可能是不必要的，因此我们只需要满足某个长度为τ的时间跨度，即使用观测序列$x_{t−1},...,x_{t-τ}$。当下获得的最直接的好处就是参数的数量总是不变的，至少在t >τ时如此，这种模型被称为自回归模型(autoregressive models)，因为它们是对自己执行回归。

如图所示，是保留一些对过去观测的总结$h_t$，并且同时更新预测$\hat{x}_t$和总结ht。这就产生了基于$\hat{x}_t = P(x_t | ht)$估计$x_t$，以及公式$h_t =g(h_{t−1},x_{t−1})$更新的模型。由于$h_t$从未被观测到，这类模型也被称为隐变量自回归模型(latent auto regressive models)。

![1716620345156](.\typora-user-images\1716620345156.png)

总结:

- 序列模型的估计需要专门的统计工具，两种较流行的选择是自回归模型和隐变量自回归模型。
- 对于时间是向前推进的因果模型，正向估计通常比反向估计更容易。
- 对于直到时间步t的观测序列，其在时间步t+k的预测输出是"k步预测"。随着我们对预测时间k值的
  增加，会造成误差的快速累积和预测质量的极速下降。

### 文本预处理

文本的常见预处理步骤:

1. 将文本作为字符串加载到内存中。
2. 将字符串拆分为词元(如单词和字符)。
3. 建立一个词表，将拆分的词元映射到数字索引。
4. 将文本转换为数字索引序列，方便模型操作。

#### 词元化

词元(token)是文本的基本单位。

```python
def tokenize(lines, token='word'): #@save    
    """将文本行拆分为单词或字符词元"""    
    if token == 'word':        
        return [line.split() for line in lines]    
    elif token == 'char':        
        return [list(line) for line in lines]    
    else:        
        print('错误：未知词元类型：' + token)
```

#### 词表

词表(vocabulary)，用来将字符串类型的词元映射到从0开始的数字索引中。我们先将训练
集中的所有文档合并在一起，对它们的唯一词元进行统计，得到的统计结果称之为语料(corpus)

语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元"<unk>"。

我们可以选择增加一个列表，用于保存那些被保留的词元，例如：填充词元("<pad>")；

序列开始词元(`<bos>`)；序列结束词元(`<eos>`)。

### 语言模型和数据集

#### 拉普拉斯平滑

假设长度为T的文本序列中的词元依次为$x_1,x_2,...,x_T$。于是，$x_t(1≤t≤T)$可以被认为是文
本序列在时间步t处的观测或标签。在给定这样的文本序列时，语言模型(language model)的目标是估计序
列的联合概率
$$
P(x_1,x_2,...,x_T).
$$
例如，只需要一次抽取一个词元$x_t∼P(x_t |x_{t−1},...,x_1)$，一个理想的语言模型就能够基于模型本身生成自
然文本。

为了训练语言模型，我们需要计算单词的概率，以及给定前面几个单词后出现某个单词的条件概率。这些概
率本质上就是语言模型的参数。

$$P(deep,learning,is,fun) = P(deep)P(learning | deep)P(is | deep,learning)P(fun | deep,learning,is).$$

训练数据集中词的概率可以根据给定词的相对词频来计算,例如可以通过计算某个单词在数据集中出现的次数除以总单词数可以预测该词出现的概率

$$
\hat{P}(learning | deep) =\frac{n(deep, learning)}{n(deep)} 
$$
其中n(x)和n(x,x′)分别是单个单词和连续单词对的出现次数。有一个问题是n(x,x′)在数据集中出现的次数远小于n(x)，比如连续单词对"deep learning"的出现频率要低得多，所以估计这类单词正确的概率要困难得多。如果数据集很小，或者单词非常罕见，那么这类单词出现一次的机会可能都找不到。

解决上述问题的常见的策略是执行某种形式的拉普拉斯平滑(Laplace smoothing)，

$$
\hat{P}(x) = \frac{n(x)+ϵ1/m}{n+ϵ1} \\
\hat{P’}(x'|x) = \frac{n(x,x')+ϵ2\hat{P}(x')}{n(x)+ϵ2}\\
\hat{P''}(x''|x,x') = \frac{n(x,x',x'')+ϵ3\hat{P}(x'')}{n(x,x')+ϵ3}
$$
n表示训练集中的单词总数，用m表示唯一单词的数量,，ϵ1,ϵ2和ϵ3是超参数。以ϵ1为例：当ϵ1 =0时，不应用平滑；当ϵ1接近正无穷大时，ˆ P(x)接近均匀概率分布1/m。

然而，这样的模型很容易变得无效，原因如下：首先，我们需要存储所有的计数；其次，这完全忽略了单词
的意思。例如，"猫"(cat)和"猫科动物"(feline)可能出现在相关的上下文中，但是想根据上下文调整这
类模型其实是相当困难的。最后，长单词序列大部分是没出现过的，因此一个模型如果只是简单地统计先前
"看到"的单词序列频率，那么模型面对这种问题肯定是表现不佳的。

#### 自然语言统计

$$
P(x1,x2,x3,x4) = P(x1)P(x2)P(x3)P(x4)\\
  P(x1,x2,x3,x4) = P(x1)P(x2 | x1)P(x3 | x2)P(x4 | x3)\\
P(x1,x2,x3,x4) = P(x1)P(x2 | x1)P(x3 | x1,x2)P(x4 | x2,x3).
$$

涉及一个、两个和三个变量的概率公式分别被称为一元语法(unigram)、二元语法(bigram)和三
元语法(trigram)模型。

最流行的词看起来很无聊，这些词通常被称为停用词(stop words)，因此可以被过滤掉。尽管如此，它们本身仍然是有意义的，

#### 读取长序列数据

当序列变得太长而不能被模型一次性全部处理时，我们希望拆分这样的序列方便模型读取,任意长的序列可以
被我们划分为具有相同时间步数的子序列,当训练我们的神经网络时,这样的小批量子序列将被输入到模型中。

![1716626563235](.\typora-user-images\1716626563235.png)

分割文本时，**不同的偏移量会导致不同的子序列**

##### 随机采样

在随机采样中，每个样本都是在原始的长序列上任意捕获的子序列。在迭代过程中，来自两个相邻的、随机
的、小批量中的子序列不一定在原始序列上相邻。

##### 顺序分区

在迭代过程中，除了对原始序列可以随机抽样外，我们还可以保证两个相邻的小批量中的子序列在原始序列
上也是相邻的。这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序，因此称为顺序分区。

 ### 循环神经网络

隐变量模型：
$$
P(x_t | x_{t−1},...,x_1) ≈ P(x_t | h_{t−1}),
$$
$h_{t−1}$是隐状态(hidden state)，也称为隐藏变量(hidden variable)，它存储了到时间步t−1的序列信
息。通常，我们可以基于当前输入$x_t$和先前隐状态$h_{t−1}$来计算时间步t处的任何时间的隐状态：
$$
h_t = f(x_t,h_{t−1})
$$
循环神经网络(recurrent neural networks，RNNs)是具有隐状态的神经网络。

下图展示了循环神经网络在三个相邻时间步的计算逻辑。在任意时间步t，隐状态的计算可以被视为：

1. 拼接当前时间步t的输入$X_t$和前一时间步t−1的隐状态$H_{t−1}$；
2. 将拼接的结果送入带有激活函数ϕ的全连接层。全连接层的输出是当前时间步t的隐状态Ht。

![1716627390424](.\typora-user-images\1716627390424.png)

模型参数是$W_{xh}$和$W_{hh}$的拼接，以及$bh$的偏置,当前时间步t的隐状态$Ht$ 将参与计算下一时间步t+1的隐状态$H_{t+1}$。而且Ht还将送入全连接输出层，用于计算当前时间步t的输出$O_t$

#### 基于循环神经网络的字符级语言模型

使用字符级语言模型(character‐level language model)，将文本词元化为字符而不是单词

![1716627671731](.\typora-user-images\1716627671731.png)

输入序列和标签序列分别为"machin"和"achine",在训练过程中，我们对每个时间步的输出层的输出进行`softmax`操作，然后利用交叉熵损失计算模型输出和标签之间的误差。

#### 困惑度(Perplexity)

如果想要压缩文本，我们可以根据当前词元集预测的下一个词元。一个更好的语言模型应该能让我们更准确地预测下一个词元。因此，它应该允许我们在压缩序列时花费更少的比特。所以我们可以通过一个序列中所有的n个词元的交叉熵损失的平均值来衡量：
$$
\frac{1}{n}\sum_{t=1}^n−logP(x_t | x_{t−1},...,x_1),
$$
其中P由语言模型给出，xt是在时间步t从该序列中观察到的实际词元。这使得不同长度的文档的性能具有了
可比性。由于历史原因，自然语言处理的科学家更喜欢使用一个叫做困惑度(perplexity)的量。简而言之，
它是上述公式的指数：
$$
exp(\frac{1}{n}\sum_{t=1}^n−logP(x_t | x_{t−1},...,x_1))
$$
困惑度的最好的理解是"下一个词元的实际选择数的调和平均数"

- 在最好的情况下，模型总是完美地估计标签词元的概率为1。在这种情况下，模型的困惑度为1。
- 在最坏的情况下，模型总是预测标签词元的概率为0。在这种情况下，困惑度是正无穷大。
- 在基线上，该模型的预测是词表的所有可用词元上的均匀分布。在这种情况下，困惑度等于词表中唯一
  词元的数量。

#### 梯度裁剪

## 现代循环神经网络

### 门控循环单元(GRU)

门控循环单元与普通的循环神经网络之间的关键区别在于:

GRU支持隐状态的门控。这意味着模型有专门的机制来确定应该何时更新隐状态，以及应该何时重置隐状态。

#### 重置门和更新门

重置门:决定是否忽略长短期依赖信息，即决定是否忽略之前的状态，只关注当前的输入。

更新门:决定有多少之前的记忆要保留，多少新信息要加入。更新门帮助网络决定在当前时间步应该更新多少之前的隐藏状态。

![1716887915302](.\typora-user-images\1716887915302.png)

门控循环单元的数学表达
$$
R_t =σ(X_tW_{xr} +H_{t−1}W_{hr} +b_r)\\
Z_t = σ(X_tW_{xz} +H_{t−1}W_{hz} +b_z)
$$
$H_{t−1}∈R_{n× h}$是上一个时间步的隐状态,$Wxr$,$Wxz ∈ Rd× h$ 和$Whr$,$Whz ∈ Rh× h$是权重参数，$br$,$bz ∈ R1× h$是偏置参数

我们使用sigmoid函数将输入值转换到区间(0,1)。

#### 候选隐状态

将重置门与常规隐状态更新机制集成，得到在时间步t的候选隐状态(candidate
hidden state)$\tilde{H}_t ∈ R_{n× h}$
$$
\tilde{H}_t =tanh(X_tW_{xh} +(R_t ⊙H_{t−1})W_{hh} +b_h)
$$
符号⊙是Hadamard积(按元素乘积)运算符。在这里，我们使用tanh非线性激活函数来确保候选隐状态中的值保持在区间(−1,1)中。

![1716888695331](.\typora-user-images\1716888695331.png)

观察上述公式可以发现，$R_t$和$H_{t−1} $的元素相乘可以减少以往状态的影响。每当重置门$R_t$中的项接近1时，上式就恢复成一个普通的循环神经网络。当重置门$R_t$中有的项接近0时，候选隐状态是以$X_t$作为输入的多层感知机的结果。因此，任何预先存在的隐状态都会被重置为默认值。

#### 隐状态

上述的计算结果只是候选隐状态，我们仍然需要结合更新门$Z_t$的效果。这一步确定新的隐状态$H_t∈R_{n× h}$在多大程度上来自旧的状态$H_{t−1}$和新的候选状态$\tilde{H}_t$。更新门Zt仅需要在$H_{t−1}$和$\tilde{H}_t$之间进行按元素的凸组合就可以实现这个目标。这就得出了门控循环单元的最终更新公式：
$$
H_t =Z_t ⊙H_{t−1} +(1−Z_t)⊙ \tilde{H}_t
$$
当更新门Zt接近1时，模型就倾向只保留旧状态,相反，当Zt接近0时，新的隐状态Ht就会接近候选隐状态$\tilde{H}_t$,这些设计可以帮助我们处理循环神经网络中的梯度消失问题，并更好地捕获时间步距离很长的序列的依赖关系。

![1716889459862](.\typora-user-images\1716889459862.png)

总之，门控循环单元具有以下两个显著特征：

- 重置门有助于捕获序列中的短期依赖关系；
- 更新门有助于捕获序列中的长期依赖关系。

### 长短期记忆网络(LSTM)

#### 门控记忆元

记忆元是隐状态的一种特殊类型，它们与隐状态具有相同的形状，其设计目的是用于记录附加的信息。为了控制记忆元，我们需要许多门。

- 其中一个门用来从单元中输出条目，我们将其称为输出门(output gate)。
- 另外一个门用来决定何时将数据读入单元，我们将其称为输入门(input gate)。
- 我们还需要一种机制来重置单元的内容，由遗忘门(forget gate)来管理

和之前的GRU一样，当前时间步的输入和前一个时间步的隐状态作为数据送入长短期记忆网络的门中，它们由三个具有sigmoid激活函数的全连接层处理，以计算输入门、遗忘门和输出门的值。因此，这三个门的值都在(0,1)的范围内。

![1716890486222](.\typora-user-images\1716890486222.png)

三个门的数学表达如下:
$$
I_t = σ(X_tW_{xi} +H_{t−1}W_{hi} +b_i)\\
F_t = σ(X_tW_{xf} +H_{t−1}W_{hf} +b_f)\\
O_t =σ(X_tW_{xo} +H_{t−1}W_{ho} +b_o)
$$

#### 候选记忆元

候选记忆元(candidate memory cell)$˜ Ct ∈Rn× h$计算与上面描述的三个门的计算类似，但是使用tanh函数作为激活函数，函数的值范围为(−1,1)。

$$
\tilde{C}_t = tanh(X_tW_{xc} +H_{t−1}W_{hc} +b_c)
$$
![1716902613954](.\typora-user-images\1716902613954.png)

#### 记忆元

在长短期记忆网络中，输入门和遗忘门用来控制输入和遗忘(或跳过):输入门It控制采用多少来自$\tilde{C}_ t$的新数据，而遗忘门$F_t$控制保留多少过去的记忆元$C_{t−1}∈R_{n× h}$的内容。
$$
C_t = F_t ⊙C_{t−1} +I_t ⊙ \tilde{C}_ t.
$$
如果遗忘门始终为1且输入门始终为0，则过去的记忆元Ct−1将随时间被保存并传递到当前时间步。引入这种
设计是为了缓解梯度消失问题，并更好地捕获序列中的长距离依赖关系。

![1716902949466](.\typora-user-images\1716902949466.png)

#### 隐状态

最后，我们需要定义如何计算隐状态$Ht∈Rn× h$,在长短期记忆网络中，它仅仅是记忆元的tanh的门控版本。这就确保了Ht的值始终在区间(−1,1)内：
$$
H_t =O_t⊙tanh(C_t).
$$
只要输出门接近1，我们就能够有效地将所有记忆信息传递给预测部分，而对于输出门接近0，我们只保留记
忆元内的所有信息，而不需要更新隐状态。

总结:

长短期记忆网络是典型的具有重要状态控制的**隐变量自回归模型**。多年来已经提出了其许多变体，例如，多
层、残差连接、不同类型的正则化。然而，由于序列的长距离依赖性，训练长短期记忆网络和其他序列模型
(例如门控循环单元)的成本是相当高的。

### 深度循环神经网络

![1716903824370](.\typora-user-images\1716903824370.png)

我们可以将深度架构中的函数依赖关系形式化，这个架构是由上图中描述了L个隐藏层构成。

第l层的隐状态可描述为:

$$
H^{(l)}_t=ϕ(H^{(l−1)}_tW^{(l)}_{xh} +H^{(l−1)}_tW^{(l)}_{hh} +b^{(l)}_h )
$$
$W^{(l)}_{xh}$,$W^{(l)}_{hh}$,$b^{(l)}_h$都是第l个隐藏层的模型参数。

最后，输出层的计算仅基于第l个隐藏层最终的隐状态：
$$
O_t =H^{(L)}_tW_{hq} +b_q,
$$
里面的参数都是输出层的模型参数

### 双向循环神经网络

![1716986597836](.\typora-user-images\1716986597836.png)

双向循环网络不只是在前向模式下"从第一个词元开始运行"的循环神经网络，也是从最后一个词元开始从后向
前运行"的循环神经网络。双向循环神经网络(bidirectional RNNs)添加了反向传递信息的隐藏层，以便更灵活地处理此类信息，上图是一个具有一个隐藏层的双向循环神经网络的架构

对于任意时间步t，给定一个小批量的输入数据$X_t∈R_{n× d}$(样本数n，每个示例中的输入数d)，并且令隐藏
层激活函数为ϕ。在双向架构中，我们设该时间步的前向和反向隐状态分别为$\overrightarrow{H}_t∈R_{n× h}$和$\overleftarrow{H}_t∈R_{n× h}$，其中h是隐藏单元的数目。前向和反向隐状态的更新如下：
$$
\overrightarrow{H}_t =ϕ(X_tW^{(f)}_{xh} +\overrightarrow{H}_{t−1}W^{(f)}_{hh} +b^{(f)}_h)\\
\overleftarrow{H}t =ϕ(X_tW^{(f)}_{xh} +\overleftarrow{H}_{t+1}W^{(f)}_{hh} +b^{(f)}_h)
$$
接下来，将前向隐状态−→ Ht和反向隐状态←− Ht连接起来，获得需要送入输出层的隐状态Ht∈Rn× 2h。该信息作为输入传递到下一个双向层。最后，输出层计算得到的输出为Ot ∈Rn× q(q是输出单元的数目)：
$$
O_t =H_tW_{hq} +b_q
$$
双向循环神经网络的缺陷:

- 模型在预测下一个词元时，无法知道下一个词元的下文是什么，所以将不会得到很好的精度。具体地说，在训练期间，我们能够利用过去和未来的数据来估计现在空缺的词；而在测试期间，我们只有过去的数据，因此精度将会很差。
- 另一个严重问题是，双向循环神经网络的计算速度非常慢。其主要原因是网络的前向传播需要在双向层中进行前向和后向递归，并且网络的反向传播还依赖于前向传播的结果。因此，梯度求解将有一个非常长的链。

### 机器翻译与数据集

机器翻译(machine translation)指的是将序列从一种语言自动翻译成另一种语言。

一般步骤:

#### 下载和预处理数据集

#### 词元化

在机器翻译中，我们更喜欢单词级词元化(最先进的模型可能使用更高级的词元化技术)

单词级词元化和字符级词元化是自然语言处理(NLP)中文本处理的两种不同方法，它们在处理文本数据时有不同的侧重点和应用场景：

1. **单词级词元化(Word-level tokenization)**：
   - 这是最常用的文本处理方法之一，涉及将文本分割成单词或词汇单元。
   - 单词级词元化通常使用空格和标点符号作为分隔符，将文本拆分成可识别的单词。
   - 这种方法适用于词汇量大、词汇重复率高的语言，如英语，因为它可以利用预先定义好的词汇表(vocabulary)来处理文本。
   - 优点是处理速度快，模型参数数量相对较少，易于实现。
   - 缺点是对于词汇量较小或不规则的词汇(如专有名词、新词)处理能力有限，可能导致信息丢失。

2. **字符级词元化(Character-level tokenization)**：
   - 字符级词元化是将文本分割成单个字符或字符组合的过程。
   - 这种方法不依赖于预先定义的词汇表，而是直接从字符级别学习语言的模式。
   - 字符级词元化适用于词汇量较小或存在大量未知词汇的语言，以及那些字符重复率高的语言。
   - 优点是灵活性高，能够处理任意词汇，包括未知词汇和拼写错误。
   - 缺点是模型参数数量多，计算复杂度高，可能导致训练时间长。

在实际应用中，选择单词级还是字符级词元化取决于特定任务的需求和语言特性。例如，在处理英语文本时，单词级词元化通常是首选，因为英语的词汇量大，且存在大量的共享词汇。而在处理中文、日文等没有明显单词分隔的语言时，字符级词元化可能更为合适，因为这些语言的文本不以空格分隔单词。

另外，一些现代NLP模型，如BERT，采用了一种称为"Word Piece"的词元化方法，它结合了单词级和字符级词元化的优点，能够更好地处理未知词汇和不规则表达。

#### 词表

机器翻译数据集是由源语言和目标语言的语言对组成，可以分别为源语言和目标语言构建两个词表。使用单词级词元化时，词表大小将明显大于使用字符级词元化时的词表大小。为了解决这个问题，根据词元出现频率做如下规定：

- 次数少于2次的低频率词元视为相同的未知(`<unk>`)词元
- 序列的开始词元(`<bos>`)和结束词元(`<eos>`)
- 在小批量时用于将序列填充到相同长度的填充词元(`<pad>`)

#### 加载数据集

在机器翻译中，每个样本都是由源和目标组成的文本序列对，其中的每个文本序列可能具有不同的长度。但是，语言模型中的序列样本都有一个固定的长度，为了提高计算效率，我们仍然可以通过截断和填充方式实现一次只处理一个小批量的文本序列。

- 如果文本序列的词元数目少于num_steps时，我们将继续在其末尾添加特定的`<pad>`词元，直到其长度达到num_steps；
- 反之，我们将截断文本序列时，只取其前num_steps 个词元，并且丢弃剩余的词元。

#### 训练模型

### 编码器-解码器架构

机器翻译的输入和输出都是长度可变的序列。为了处理这种类型的输入和输出，我们可以设计一个包含两个主要组件的架构：

- 第一个组件是一个编码器(encoder)：它接受一个长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。
- 第二个组件是解码器(decoder)：它将固定形状的编码状态映射到长度可变的序列。

这被称为编码器-解码器(encoder‐decoder)架构

![1717051133230](.\typora-user-images\1717051133230.png)

### 序列到序列学习(seq2seq)

循环神经网络编码器使用长度可变的序列作为输入，将其转换为固定形状的隐状态。换言之，输入序列的信息被编码到循环神经网络编码器的隐状态中。为了连续生成输出序列的词元，独立的循环神经网络解码器是基于输入序列的编码信息和输出序列已经看见的或者生成的词元来预测下一个词元。

![1717053117831](.\typora-user-images\1717053117831.png)

上图中的`<eos>`表示序列结束词元。一旦输出序列生成此词元，模型就会停止预测。`<bos>`表示序列开始词元，它是解码器的输入序列的第一个词元。

#### 编码器

编码器将长度可变的输入序列转换成形状固定的上下文变量c，并且将输入序列的信息在该上下文变量中进行编码。可以使用循环神经网络来设计编码器。

在时间步t，循环神经网络将词元xt的输入特征向量xt和$h_{t−1}$(即上一时间步的隐状态)转换为ht(即当前步的隐状态)。
$$
h_t = f(x_t,h_t−1)
$$
编码器通过选定的函数q，将所有时间步的隐状态转换为上下文变量：

$$
c =q(h_1,...,h_T).
$$
上下文变量仅仅是输入序列在最后时间步的隐状态$h_T$,我们可以使用双向循环神经网络构造编码器，其中隐状态依赖于两个输入子序列，两个子序列是由隐状态所在的时间步的位置之前的序列和之后的序列(包括隐状态所在的时间步)，因此隐状态对整个序列的信息都进行了编码

这里实现双向循环神经网络使用了嵌入层，它的主要作用是将离散的单词或标记(tokens)转换为连续的向量表示。嵌入层的权重是一个矩阵，其行数等于输入词表的大小(vocab_size)，其列数等于特征向量的维度(embed_size)。对于任意输入词元的索引i，嵌入层获取权重矩阵的第i行(从0开始)以返回其特征向量。

#### 解码器

编码器输出的上下文变量$\mathbf{c}$ 对整个输入序列$x_1, \ldots, x_T$进行编码，解码器输出$y_{t'}$的概率取决于先前的输出子序列 𝑦1,…,𝑦𝑡′−1和上下文变量$\mathbf{c}$， 即$P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c})$。

为了对上述解码器描述进行建模，我们使用另一种循环神经网络进行建模。循环神经网络将来自上一时间步的输出$y_{t^\prime-1}$ 和上下文变量$\mathbf{c}$作为其输入， 然后在当前时间步将它们和上一隐状态 $\mathbf{s}_{t^\prime-1}$转换为隐状态$\mathbf{s}_{t^\prime}$

$$
\mathbf{s}_{t^\prime} = g(y_{t^\prime-1}, \mathbf{c}, \mathbf{s}_{t^\prime-1})
$$
获得解码器的隐状态之后， 我们可以使用输出层和softmax操作 来计算在时间步$t^\prime$时输出$y_{t^\prime}$的条件概率分布 

$P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c})$

![1717134819176](.\typora-user-images\1717134819176.png)

#### 损失函数

在每个时间步，解码器预测了输出词元的概率分布。类似于语言模型，可以使用softmax来获得分布，并通过
计算交叉熵损失函数来进行优化。

#### 训练

训练的时候，特定的序列开始词元(`<bos>`)和原始的输出序列(不包括序列结束词元`<eos>`)拼接在一起作为解码器的输入。这被称为强制教学(teacher forcing)，因为原始的输出序列(词元的标签)被送入解码器。或者，将来自上一个时间步的预测得到的词元作为解码器的当前输入。

#### 预测

为了采用一个接着一个词元的方式预测输出序列，每个解码器当前时间步的输入都将来自于前一时间步的预
测词元。与训练类似，序列开始词元(`<bos>`)在初始时间步被输入到解码器中。该预测过程如图9.7.3所
示，当输出序列的预测遇到序列结束词元(`<eos>`)时，预测就结束了。

#### 预测序列的评估

`BLEU`值(Bilingual Evaluation Understudy)是自然语言处理(NLP)领域中用于评估机器翻译质量的一种度量标准。它通过计算机器翻译输出与一组参考翻译之间的重叠程度来工作。`BLEU`分数范围从0到1，其中1表示完美的匹配，0表示没有重叠。
$$
\exp\left(\min\left(0, 1 - \frac{\mathrm{len}*{\text{label}}}{\mathrm{len}*{\text{pred}}}\right)\right) \prod_{n=1}^k p_n^{1/2^n}
$$
其中$len_{label}$表示标签序列中的词元数和$len_{pred}$表示预测序列中的词元数，k是用于匹配的最长的n元语法。另外，用$p_n$表示n元语法的精确度，它是两个数量的比值：第一个是预测序列与标签序列中匹配的n元语法的数
量，第二个是预测序列中n元语法的数量的比率。

给定标签序列$A$、$B$、$C$、$D$、$E$、$F$ 和预测序列$A$、$B$、$B$、$C$、$D$， 我们有$p_1 = 4/5$、$p_2 = 3/4$、$p_3 = 1/3$和$p_4 = 0$

`BLEU`分数是一种快速且广泛接受的评估机器翻译质量的方法，尽管它有一些局限性，比如它不能很好地评估语义的准确性和流畅性，但它仍然是研究和工业界中常用的评估标准之一。在实际使用中，通常会结合其他指标(如METEOR、ROUGE等)来更全面地评估翻译质量。

###  束搜索

之前我们预测的时候，我们是逐个预测输出序列，直到预测序列中出现特定的序列结束词元`<eos>`

#### 贪心搜索

对于输出序列的每一时间步$t'$， 我们都将基于贪心搜索从$\mathcal{Y}$中找到具有最高条件概率的词元，即：
$$
y_{t'} = \operatorname*{argmax}_{y \in \mathcal{Y}} P(y \mid y_1, \ldots, y*{t'-1}, \mathbf{c})
$$
一旦输出序列包含了`<eos>`或者达到其最大长度$T'$，则输出完成

![1717138046852](.\typora-user-images\1717138046852.png)

如图，在预测输出序列"A""B""C"和`<eos>`。这个输出序列的条件概率是0.5×0.4×0.4×0.6=0.048。

贪心搜索的问题是贪心搜索无法保证得到最优序列。

![1717137954964](.\typora-user-images\1717137954964.png)

在图9.8.2中的时间步4生成每个词元的条件概率也不同于图9.8.1中的条件概率。结果，图9.8.2中的输出序列"A""C""B"和""的条件概率为0.5×0.3×0.6×0.6=0.054，这大于图9.8.1中的贪心搜索的条件概率。

这个例子说明：**贪心搜索获得的输出序列不一定最佳**。

#### 穷举搜索

穷举搜索(exhaustive search)：穷举地列举所有可能的输出序列及其条件概率，然后计算输出条件概率最高的一个。

#### 束搜索

如果精度最重要，则显然是穷举搜索。如果计算成本最重要，则显然是贪心搜索。而束搜索的实际应用则介于这两个极端之间。

*束搜索*(beam search)是贪心搜索的一个改进版本。 它有一个超参数，名为*束宽*(beam size)$k$。 在时间步$1$，我们选择具有最高条件概率的$k$个词元。 这$k$个词元将分别是$k$个候选输出序列的第一个词元。 在随后的每个时间步，基于上一时间步的$k$个候选输出序列， 我们将继续从$k\left|\mathcal{Y}\right|$个可能的选择中 挑出具有最高条件概率的$k$个候选输出序列。

![1717138254319](.\typora-user-images\1717138254319.png)

束搜索计算
$$
\frac{1}{L^\alpha} \log P(y_1, \ldots, y_{L}\mid \mathbf{c}) = \frac{1}{L^\alpha} \sum_{t'=1}^L \log P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c})
$$
其中$L$是最终候选序列的长度， 𝛼通常设置为$0.75$。 因为一个较长的序列在上述式子的求和中会有更多的对数项， 因此分母中的$L^\alpha$用于惩罚长序列。

束搜索的计算量为$\mathcal{O}(k\left|\mathcal{Y}\right|T')$， 这个结果介于贪心搜索和穷举搜索之间。 实际上，贪心搜索可以看作一种束宽为$1$的特殊类型的束搜索。 通过灵活地选择束宽，束搜索可以在正确率和计算代价之间进行权衡。

## 注意力机制

### Q&K&V

- 查询(Query): 指的是查询的范围，自主提示，即主观意识的特征向量

- 键(Key): 指的是被比对的项，非自主提示，即物体的突出特征信息向量

- 值(Value) :  则是代表物体本身的特征向量，通常和Key成对出现

  注意力机制是通过Query与Key的注意力汇聚(给定一个 Query，计算Query与 Key的相关性，然后根据Query与Key的相关性去找到最合适的 Value)实现对Value的注意力权重分配，生成最终的输出结果
  ![image-20240425202647313](.\typora-user-images\image-20240425202647313.png)

### 注意力汇聚:

### Nadaraya-Watson 核回归

注意力机制的主要成分是:查询(自主提示)和键(非自主提示)之间的交互形成了**注意力汇聚**；注意力汇聚有选择地聚合了值(感官输入)以生成最终的输出。

#### 平均汇聚

基于平均汇聚来计算所有训练样本输出值的平均值：
$$
f(x) = \frac{1}{n}\sum_{i=1}^{n} \hat{y}_i
$$
![1717297609087](.\typora-user-images\1717297609087.png)

根据平均汇聚得到的真实函数f（"Truth"）和预测函数（"Pred"）相差很大。

#### 非参数注意力汇聚

由于平均汇聚忽略了输入xi，所以提出了基于Nadaraya和Watson 的核回归，根据输入的位置对输出yi进行加权:
$$
f(x) = \sum_{i=1}^n\frac{K(x-x_i)}{\sum_{j=1}^{n}(K(x-x_j))}
$$
其中K是核（kernel）,上述公式也被称为Nadaraya-Watson核回归（Nadaraya‐Watson kernel
regression）

受此启发，我们可以从注意力机制框架的角度重写上述公式，成为一个更加通用的注意力汇聚(attention pooling)公式:
$$
f(x) =\sum_{i=1}^{n}\alpha(x,x_i)y_i
$$
其中x是查询，(xi , yi)是键值对。注意力汇聚是yi的加权平均**。将查询x和键xi之间的关系建模为注意力权重**,这个权重将被分配给每一个对应值yi。 **对于任何查询，模型在所有键值对注意力权重都是一个有效的概率分布:它们是非负的，并且总和为1。**

如果一个键xi越是接近给定的查询x，那么分配给这个键对应值yi的注意力权重就会越大，也就"获得了更多的注意力"。

为了更好地理解注意力汇聚，下面考虑一个高斯核（Gaussiankernel），其定义为：
$$
K(u) = \frac{1}{\sqrt{2\pi}} exp(\frac{−u^2}
2 ).
$$
将高斯核代入注意力汇聚公式可得到如下结果
$$
\begin{aligned} f(x) &=\sum_{i=1}^n \alpha(x, x_i) y_i\ \\ &= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}(x - x_i)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}(x - x_j)^2\right)} y_i = \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}(x - x_i)^2\right) y_i. \end{aligned}
$$
![image-20240425213125542](.\typora-user-images\image-20240425213125542.png)

基于这个非参数的注意力汇聚模型来绘制预测结果。从绘制的结果会发现新的模型预测线是平滑的，并且比平均汇聚的预测更接近真实。

#### 带参数注意力汇聚

非参数的Nadaraya‐Watson核回归具有一致性（consistency）的优点:如果有足够的数据，此模型会收敛到 最优结果。尽管如此，我们还是可以轻松地将可学习的参数集成到注意力汇聚中。

与非参数注意力汇聚略有不同，在下面的查询x和键xi之间的距离乘以可学习参数w:

![image-20240425213538751](.\typora-user-images\image-20240425213538751.png)

##### 批量矩阵乘法

```python
X = torch.ones((2, 1, 4)) # 生成两个1*4的矩阵
Y = torch.ones((2, 4, 6)) # 生成2个4*6的矩阵
# 批量矩阵乘法
print(torch.bmm(X, Y).shape) # torch.Size([2, 1, 6])
print(torch.Size([2, 1, 6])) # torch.Size([2, 1, 6])
```

在注意力机制的背景中，我们可以使用小批量矩阵乘法来计算小批量数据中的加权平均值

基于带参数的注意力汇聚，使用小批量矩阵乘法

![image-20240425215641641](.\typora-user-images\image-20240425215641641.png)

可以看到随着迭代次数变多，损失是下降的，然后就是来观察真实值和预测值的拟合情况

![image-20240425215818538](.\typora-user-images\image-20240425215818538.png)

虽然拟合度变高了，但是曲线不是很平滑，因为与非参数的注意力汇聚模型相比，带参数的模型加入可学习的参数后，曲线在注意力权重较大的区域变得更不平滑。

### 注意力评分函数

在注意力汇聚中，使用高斯核来对查询和键之间的关系建模。**高斯核指数部分可以视为注意力评分函数** ，简称评分函数，然后把这个函数的输出结果输入到softmax函数中进行运算。通过上述步骤，将得到与键对应的值的概率分布(**即注意力权重**)。最后，**注意力汇聚的输出就是基于这些注意力权重的值的加权和**。下图简单的描绘了这一过程，其中a表示注意力评分函数

![image-20240428135706873](.\typora-user-images\image-20240428135706873.png)

如上图所示，选择不同的注意力评分函数a会导致不同的注意力汇聚操作，这里我们介绍两个流行的评分函数:

 #### 掩蔽softmax操作

如上面提到的，softmax操作用于输出一个概率分布作为注意力权重。在某些情况下，并非所有的值都应该 被纳入到注意力汇聚中。例如有些文本序列含有无意义的词元，这种信息就不能作为有效值被处理，所以应该丢弃这种无意义的值。可以指定一个有效序列长度(即词元的个数),以便在计算softmax时过滤掉超出指定范围的位置

#### 加性注意力

一般来说，当查询和键是不同长度的矢量时，可以使用加性注意力作为评分函数。给定查询$q∈R_ q$和键$k∈ R_ k$,加性注意力(additive attention)的评分函数为
$$
a(q,k)=w_v^Ttanh(w_q^q+w_k^k)∈R
$$

#### 缩放点积注意力 

使用点积可以得到计算效率更高的评分函数，但是点积操作要求查询和键具有相同的长度d。假设查询和键 的所有元素都是独立的随机变量，并且都满足零均值和单位方差，那么两个向量的点积的均值为0，方差为d。 为确保无论向量长度如何，点积的方差在不考虑向量长度的情况下仍然是1，我们再将点积除以$\sqrt{d}$，则缩放点积注意力评分函数为：
$$
a(q,k) = q^Tk/\sqrt{d}
$$
在实践中，我们通常从小批量的角度来考虑提高效率，例如基于n个查询和m个键－值对计算注意力，其中查询和键的长度为d，值的长度为v。查询$Q∈R_{n×d}$、键$K∈R_{m×d}$和值$V∈R_{m×v}$的缩放点积注意力是：
$$
softmax(Qk^T/\sqrt{d})V ∈ R^{n×v}
.
$$
小结 

1. 将注意力汇聚的输出计算可以作为值的加权平均，选择不同的注意力评分函数会带来不同的注意力汇 聚操作。 
2. 当查询和键是不同长度的矢量时，可以使用可加性注意力评分函数。当它们的长度相同时，使用缩放的 "点－积"注意力评分函数的计算效率更高。

### Bahdanau 注意力

在预测词元时，如果不是所有输入词元都相关，模型将仅对齐(或参与)输入序列中与当前预测相关的部分。这是通过将上下文变量视为**注意力集中的输出**来实现的。

#### 模型

Bahdanau注意力模型与seq2seq中的模型相同，只不过上下文变量c在任何解码时间步t′都会被$c_{t′}$替换。假设输入序列中有T个词元， 解码时间步t ′的上下文变量是注意力集中的输出：
$$
c_{t'}=\sum_{t=1}^{T}\alpha(s_{t'},h_t)h_t
$$
其中，时间步${t′−1}$时的解码器隐状态$s_{t ′−1}$是查询，编码器隐状态ht既是键也是值，注意力权重α是使用加性注意力打分函数计算的。

![image-20240429133756710](.\typora-user-images\image-20240429133756710.png)

小结 

- 在预测词元时，如果不是所有输入词元都是相关的，那么具有Bahdanau注意力的循环神经网络编码 器‐解码器会有选择地统计输入序列的不同部分。这是通过将上下文变量视为加性注意力池化的输出来 实现的。 
- 在循环神经网络编码器‐解码器中，Bahdanau注意力将上一时间步的解码器隐状态视为查询，在所有时 间步的编码器隐状态同时视为键和值。

### 多头注意力

与其只使用单独一个注意力汇聚，我们可以用独立学习得到的h组不同的线性投影(linear projections) 来变换查询、键和值。然后，这h组变换后的查询、键和值将并行地送到注意力汇聚中。最后，将这h个注意 力汇聚的输出拼接在一起，并且通过另一个可以学习的线性投影进行变换，以产生最终输出。这种设计被称为**多头注意力**

![image-20240429144243556](.\typora-user-images\image-20240429144243556.png)

#### 模型

给定查询q ∈ R dq、键k ∈ R dk和 值v ∈ R dv，每个注意力头hi(i = 1, . . . , h)的计算方法为：
$$
h_i=f(W_i^{(q)}q,W_i^{(k)}k,W_i^{v}v)∈ R^{p×v}
$$
代表注意力汇聚的函数f可以是加性注意力也可以是点积注意力

多头注意力的输出需要经过另一个线性转换，它对应着h个头连结后的结果，因此其可学习参数是$ W_o ∈ R ^{p_o×hp_v}$：
$$
Wo [h_1...h_h]^T ∈ R_{po}
$$
基于这种设计，每个头都可能会关注输入的不同部分，可以表示比简单加权平均值更复杂的函数。

### 自注意力和位置编码

将词元序列输入注意力池化中，以便同一组词元同时充当查询、键和值。具体来说，每个 查询都会关注所有的键－值对并生成一个注意力输出。由于查询、键和值来自同一组输入，因此被称为 自注 意力(self‐attention),也被称为内部注意力(intra‐attention)

给定一个由词元组成的输入序列$x_1, . . . , x_n$，其中任意$x_i ∈ R_ d(1 ≤ i ≤ n)$。该序列的自注意力输出为一个长 度相同的序列$ y_1, . . . , y_n$，其中：
$$
y_i = f(x_i ,(x_1, x_1), . . . ,(x_n, x_n)) ∈ R^ d
$$

#### 位置编码

在处理词元序列时，循环神经网络是逐个的重复地处理词元的，而自注意力则因为并行计算而放弃了顺序操作。为了使用序列的顺序信息，通过在输入表示中添加 位置编码来注入绝对的或相对的位置信息。

### Transformer

Transformer作为编码器－解码器架构的一个实例,，Transformer的编码器和解码器是基于自注意力的模块叠加而成的，源（输入）序列和目标（输出）序列的嵌入（embedding）表示将加上位置编码（positional encoding），再分别输入到编码器和解码器中。

![1717507956637](.\typora-user-images\1717507956637.png)

从宏观角度来看，Transformer的编码器是由多个相同的层叠加而成的，每个层都有两个子层（子层表示为sublayer）。第一个子层是多头自注意力（multi‐head  self‐attention）汇聚；第二个子层是基于位置的前馈网络（position wise feed‐forward network）。具体来说，在计算编码器的自注意力时，查询、键和值都来自前一个编码器层的输出。

Transformer解码器也是由多个相同的层叠加而成的，并且层中使用了残差连接和层规范化。除了编码器中
描述的两个子层之外，解码器还在这两个子层之间插入了第三个子层，称为编码器－解码器注意力（encoder
decoder attention）层。在编码器－解码器注意力中，查询来自前一个解码器层的输出，而键和值来自整个
编码器的输出。在解码器自注意力中，查询、键和值都来自上一个解码器层的输出。但是，解码器中的每个位
置只能考虑该位置之前的所有位置。这种掩蔽（masked）注意力保留了自回归（auto‐regressive）属性，确
保预测仅依赖于已生成的输出词元。

#### 基于位置的前馈网络

它的作用是对序列中每个位置的表示进行独立相同的处理，以增强模型的表达能力。对所有位置的表示进行变换时使用的是同一个多层感知机，他可以学习到输入表示的非线性变换，增强模型对复杂特征的捕捉能力，例如：输入X的形状（批量大小，时间步数或序列长度，隐单元数或特征维度）将会被一个两层的感知机转换成形状为（批量大小，时间步数，`ffn_num_outputs`）的输出张量。

#### 残差连接和层规范化

### 自然语言处理：预训练

![1717651834784](.\typora-user-images\1717651834784.png)

预训练好的文本表示可以放入各种深度学习架构，应用于不同自然语言处理任务

#### 词嵌入（`Word2Vec`）

将单词映射到实向量的技术称为词嵌入虽然独热向量很容易构建，但它们通常不是一个好的选择。一个主要原因是独热向量不能准确表达不同词之间的相似度，比如我们经常使用的"余弦相似度"，因为任意两个不同词的独热向量之间的余弦相似度为0，所以独热向量不能编码词之间的相似性。

##### 自监督的`Word2Vec`

``Word2Vec``将每个词映射到一个固定长度的向量，这些向量能更好地表达不同词之间的相似性和类比关系``Word2Vec``工具包含两个模型，即跳元模型（skip‐gram）和连续词袋（`CBOW`）

##### 跳元模型（Skip-Gram）

以文本序列"the""man""loves""his""son"为例。跳元模型假设一个词可以用来在文本序列中生成其周围的单词。假设中心词选择"loves"，并将上下文窗口设置为2,，跳元模型考虑生成上下文词"the""man""him""son"的条件概率：

![1717652324193](.\typora-user-images\1717652324193.png)

给定中心词$w_c$（词典中的索引c），生成任何上下文词$w_o$（词典中的索引o）的条件概率可以通过对向量点积的`softmax`操作来建模：
$$
P(w_o | w_c) = \frac{exp(u^⊤_ov_c)}{\sum_{i∈V} exp(u^⊤_iv_c)}
$$

##### 连续词袋（CBOW）模型

连续词袋（CBOW）模型类似于跳元模型。与跳元模型的主要区别在于，连续词袋模型假设中心词是基于其
在文本序列中的周围上下文词生成的。例如以下的示例就是对连续词袋模型的解释
$$
P("loves" | "the","man","his","son")
$$
![1717763898777](.\typora-user-images\1717763898777.png)

由于连续词袋模型中存在多个上下文词，因此在计算条件概率时对这些上下文词向量进行平均。
$$
P(w_c |(w_{o1}...w_{o2m}) ) = \frac{exp(\frac{1}{2m}u^⊤_ov_c)(v_{o1} + ...,+v_{o2m}
 )}{\sum_{i∈V} exp(\frac{1}{2m}u^⊤_iv_c)}
$$

#### 近似训练

可以看到，跳元模型和连续词袋模型的计算中都包含求和计算，不幸的是，在一个词典上（通常有几十万或数百万个单词）求和的梯度的计算成本是巨大的。为了降低上述计算复杂度，本节将介绍两种近似训练方法：负采样和分层softmax。

##### 负采样

负采样修改了原目标函数。给定中心词wc的上下文窗口，任意上下文词wo来自该上下文窗口的被认为是由下
式建模概率的事件：
$$
P(D =1|w_c,w_o) = σ(u^⊤_ov_c),  其中σ使用了sigmoid激活函数的定义：
$$
中间证明过程没看明白。。。

我们可以看到，现在每个训练步的梯度计算成本与词表大小无关，而是线性依赖于K。当将超参数K设置为
较小的值时，在负采样的每个训练步处的梯度的计算成本较小。

##### 层序Softmax

层序Softmax使用二叉树，其中树的每个叶节点表示词表V中的一个词。

![1717764885283](.\typora-user-images\1717764885283.png)

由于二叉树结构，$L(w_o)−1$大约与$O(log2|V|)$是一个数量级。当词表大小V很大时，与没有近似训
练的相比，使用分层softmax的每个训练步的计算代价显著降低。

#### 全局向量的词嵌入（`GloVe`）

上下文窗口内的词共现可以携带丰富的语义信息。例如，在一个大型语料库中，"固体"比"气体"更有可能
与"冰"共现，可以预先计算此类共现的全局语料库统计数据：这可以提高训练效率

`GloVe`（Global Vectors for Word Representation）是一种用于生成词嵌入（word embeddings）的无监督学习算法，由斯坦福大学和Google的研究人员在2014年提出。`GloVe`模型通过捕获词语在语料库中的全局统计信息来学习词向量，这些向量能够反映词语之间的语义和语法关系。

以下是`GloVe`模型的一些关键特点：

1. **全局统计信息**：`GloVe`利用整个语料库中的词语共现统计信息来构建词向量。

2. **共现矩阵**：模型首先构建一个共现矩阵，记录任意两个词语在语料库中共同出现的次数。

3. **权重预测**：`GloVe`模型试图预测词语对的共现概率，通过最小化预测概率和实际共现频率之间的差异来训练词向量。

4. **对称性**：`GloVe`模型是对称的，即对于任意两个词语 \( $w_i $) 和 \($ w_j$ \)，模型学习到的向量对 \( ($u_i$, $u_j$) \) 和 \( ($u_j$, $u_i$) \) 是相同的，其中 \( u \) 表示词向量。

5. **词向量生成**：`GloVe`模型生成的词向量捕捉了词语之间的语义相似性，使得语义上相似的词语在向量空间中更接近。

6. **易于实现**：`GloVe`模型相对容易实现，且计算效率较高。

7. **适用于多种语言**：`GloVe`模型不依赖于特定的语言结构，因此可以应用于多种语言的词嵌入生成。

8. **与其他模型的比较**：与`Word2Vec`等其他词嵌入模型相比，`GloVe`在某些任务上能够提供更好的性能，尤其是在捕捉词语的语法信息方面。

9. **开源实现**：`GloVe`模型的源代码和预训练词向量是开源的，可以在网上找到，便于研究人员和开发者使用。

10. **应用广泛**：`GloVe`词向量被广泛应用于各种自然语言处理任务，如文本分类、情感分析、命名实体识别、机器翻译等。

`GloVe`模型因其生成高质量词嵌入的能力而受到重视，这些词嵌入可以显著提高许多NLP任务的性能。随着NLP领域的不断发展，`GloVe`和其他词嵌入技术将继续是研究和应用的重要基础。

##### 带全局语料统计的跳元模型



#### 子词嵌入

在英语中，"helps""helped"和"helping"等单词都是同一个词"help"的变形形式。`Word2Vec`和`GloVe`都没有对词的内部结构进行探讨。

##### fastText模型

为了使用形态信息，fastText模型提出了一种子词嵌入方法，其中子词是一个字符n‐gram,fastText可以被认为是子词级跳元模型，而非学习词级向量表示，其中每个中心词由其子词级向量之和表示。

具体做法：

以单词"where"为例获得fastText中每个中心词的子词。

- 首先，在词的开头和末尾添加特殊字符"<"和">"，以将前缀和后缀与其他子词区分开来。
- 然后，从词中提取字符n‐gram。例如，值n=3时，我们将获得长度为3的所有子词："<wh""whe""her""ere""re>"和特殊子词"<where>"

##### 字节对编码（Byte Pair Encoding）

在fastText中，所有提取的子词都必须是指定的长度，例如3到6，因此词表大小不能预定义。为了在固定大小
的词表中允许可变长度的子词，我们可以应用一种称为字节对编码（Byte Pair Encoding，BPE）的压缩算法
来提取子词。

字节对编码（Byte Pair Encoding，简称BPE）是一种用于文本数据的子词编码方法，最初由OpenAI提出，并在GPT模型中使用。BPE的目标是将文本分解为更小的单元，这些单元可以是字节、字符或自定义的符号，以便于模型更好地处理词汇的变体和低频词汇

`BPE`算法实现的步骤

- 从单个字符开始，统计数据集中字符对的频率。
- 每次迭代中，选择频率最高的字符对并将其合并为一个新的子词单元。
- 重复合并过程，直到达到预定的词汇表大小或特定的迭代次数。

#### 词的相似性和类比任务

#### BERT

`word2vec`和`GloVe`都将相同的预训练向量分配给同一个词，而不考虑词的上下文,但是自然语言中的词有丰富的多义现象和复杂的语义，上下文无关表示具有明显的局限性。例如，在"a crane is flying"（一只鹤在飞）和"a crane driver came"（一名吊车司机来了），"crane"一词有完全不同的含义；

我们规定，词元x的上下文敏感表示是函数$f(x,c(x))$，其取决于x及其上下文$c(x)$。

流行的上下文敏感表示包括TagLM（language‐model‐augmented sequence tagger，语言模型增强的序列标记器）、CoVe（Context Vectors，上下文向量）和ELMo（Embeddings from Language Models，来自语言模型的嵌入）

例如，通过将整个序列作为输入，ELMo是为输入序列中的每个单词分配一个表示的函数。ELMo的表示将作为附加特征添加到下游任务的现有监督模型中，添加ELMo改进了六种自然语言处理任务的技术水平：情感分析、自然语言推断、语义角色标注、共指消解、命名实体识别和问答。

##### 从特定于任务到不可知任务

尽管ELMo显著改进了各种自然语言处理任务的解决方案，但每个解决方案仍然依赖于一个特定于任务的架
构。然而，为每一个自然语言处理任务设计一个特定的架构实际上并不是一件容易的事。GPT（Generative
Pre Training，生成式预训练）模型为上下文的敏感表示设计了通用的任务无关模型。与ELMo冻结预训练模型的参数不同，GPT在下游任务的监督学习过程中对预训练Transformer解码器中的所有参数进行微调。GPT在自然语言推断、问答、句子相似性和分类等12项任务上进行了评估，并在对模型架构进行最小更改的情况下改善了其中9项任务的最新水平。

然而，由于语言模型的自回归特性，GPT只能向前看（从左到右）。在"i went to the bank to deposit cash"（我去银行存现金）和"i went to the bank to sit down"（我去河岸边坐下）的上下文中，由于"bank"对其左边的上下文敏感，GPT将返回"bank"的相同表示，尽管它有不同的含义。

##### BERT：把两个最好的结合起来

如上所述，ELMo对上下文进行双向编码，但使用特定于任务的架构；而GPT是任务无关的，但是从左到右
编码上下文。BERT（来自Transformers的双向编码器表示）结合了这两个方面的优点。它对上下文进行双
向编码，并且对于大多数的自然语言处理任务只需要最少的架构改变。通过使用预训练的Transformer编码器，BERT能够基于其双向上下文表示任何词元。在下游任务的监督学习过程中，BERT在两个方面与GPT相似。

![1717936378247](.\typora-user-images\1717936378247.png)

BERT表示将被输入到一个添加的输出层中，根据任务的性质对模型架构进行最小的更改，例如预测每个词元与预测整个序列。其次，对预训练Transformer编码器的所有参数进行微调，而额外的输出层将从头开始训练。

##### 输入表示

BERT输入序列明确地表示单个文本和文本对。

- 当输入为单个文本时，BERT输入序列是特殊类别词元"<cls>"、文本序列的标记、以及特殊分隔词元"<sep>"的连结。
- 当输入为文本对时，BERT输入序列是"<cls>、第一个文本序列的标记、"<sep>"、第二个文本序列标记、以及"<sep>"的连结。

![1717937424711](.\typora-user-images\1717937424711.png)

















