## 相关名词解释

### 预训练语言模型

预训练语言模型是一种在大量无标注文本数据上进行初步训练的模型，旨在学习通用的语言表示。这些模型在预训练阶段通过无监督学习方式获取语言的丰富特征，然后可以在具体的下游任务上进行微调（fine-tuning）来提高性能。

**语言模型**：语言模型的目标是根据已知的上下文预测下一个词或句子。传统的语言模型主要有两种：从左到右的模型（left-to-right）和从右到左的模型（right-to-left）。为了更好地理解上下文，预训练语言模型逐渐发展到双向表示，即同时考虑左右上下文信息。BERT是双向表示的典型例子。

主要方法

- **自回归模型**：如GPT（Generative Pre-trained Transformer），通过从**左到右生成下一个词的方式**进行预训练。
- **掩码语言模型（MLM）**：如BERT，在预训练时**随机掩盖输入中的一些单词**，然后让模型预测这些被掩盖的单词。这种方法使模型能够利用**双向上下文信息进行训练。**
- **序列到序列模型**：如T5（Text-to-Text Transfer Transformer）,将所有任务（包括预训练和下游任务）都转换为文本到文本的形式，通过统一的框架进行训练。

### 微调

微调（Fine-tuning）是指在一个已经预训练好的模型基础上，使用特定任务的数据对模型进行进一步训练，以适应该任务的需求。微调通常用于预训练语言模型的应用过程，预训练模型如BERT、GPT等已经在大量无标注的文本数据上学习了丰富的语言表示，通过微调可以使这些模型在特定任务上表现更加优异。

微调的核心步骤

- **加载预训练模型**：首先，从预训练好的语言模型（如BERT、GPT等）开始。这些模型已经在大规模的无监督文本数据上进行了训练，具有通用的语言表示能力。
- **添加任务特定层**：根据具体的任务需求，在预训练模型的基础上添加一个或多个任务特定的层。例如，对于分类任务，可以添加一个全连接层来输出类别标签。
- **定义损失函数和优化器**：选择适合任务的损失函数和优化器。例如，对于分类任务，可以选择交叉熵损失函数。
- **使用标注数据进行训练**：使用特定任务的标注数据对模型进行训练，使模型的参数根据任务需求进行调整。这一步通常涉及多轮训练，直到模型在验证集上达到最佳性能。
- **模型评估和调优**：在训练过程中，定期评估模型的性能，根据评估结果调整模型参数和训练策略，以获得更好的结果。

### 预训练的词嵌入

预训练的词嵌入（Pre-trained Word Embeddings）是指在大规模文本数据上训练得到的向量表示，这些向量表示了单词在高维空间中的位置，使得具有相似语义的词在向量空间中相邻。预训练的词嵌入模型能够捕捉词汇的语义和语法特征，可以直接用于各种自然语言处理任务，以提高模型的性能和训练效率。

常见的预训练词嵌入模型

- **Word2Vec**：由Google提出，有两个主要的训练方法：Skip-Gram和Continuous Bag of Words (CBOW)。
  1. ​	Skip-Gram：给定一个单词预测其上下文词。
  2. ​	CBOW：给定上下文词预测中间的单词。
- **GloVe (Global Vectors for Word Representation)**：斯坦福大学，通过全局词共现矩阵来学习词嵌入。基于词汇在整个语料库中的共现频率来训练词向量。

- **FastText**：由Facebook提出，扩展了Word2Vec的思想。虑了词的内部结构，通过子词（subwords）来表示单词，使得模型能够处理未登录词和形态变化的词。

- **ELMo (Embeddings from Language Models)**：由AllenNLP提出，通过训练双向LSTM语言模型来生成上下文相关的词嵌入。每个词的表示依赖于它在句子中的上下文，因此同一个词在不同上下文中可能有不同的表示。

- **BERT**：由Google提出，通过Transformer架构进行双向预训练。生成的词嵌入是上下文相关的，并且在多个下游任务中表现出色。

### GLUE基准测试

GLUE（General Language Understanding Evaluation）基准测试是一个广泛使用的自然语言理解评估框架，旨在**通过一组多样化的任务来全面评估自然语言处理模型的表现**。GLUE基准测试涵盖了各种语言理解任务，包括文本分类、文本相似度、自然语言推理等。

**GLUE基准测试的组成**

GLUE基准测试由以下九个任务组成，每个任务都代表了不同类型的自然语言理解挑战：

- CoLA（Corpus of Linguistic Acceptability）：判断一个句子是否符合语法规则。
- **SST-2（Stanford Sentiment Treebank）**：情感分析，判断句子的情感是正面还是负面。
- **MRPC（Microsoft Research Paraphrase Corpus）**：判断两个句子是否是语义上的复述。
- **STS-B（Semantic Textual Similarity Benchmark）**：评估两个句子的语义相似度,评分范围为0到5
- **QQP（Quora Question Pairs）**：判断两个问题是否是语义上的重复问题。
- **MNLI（Multi-Genre Natural Language Inference）**：自然语言推理，判断句子对的关系（蕴涵;矛盾;中立）。
- **QNLI（Question Natural Language Inference）**：问答推理，判断给定的句子是否回答了问题。
- **RTE（Recognizing Textual Entailment）**：判断句子对的关系（蕴涵或不蕴涵）。
- **WNLI（Winograd Schema Challenge）**：基于上下文判断代词的指代对象。

### BERT

BERT 的模型架构是一个多层双向 Transformer 编码器，主要包括两个方面：**预训练**和**微调**。在预训练阶段，模型将根据不同的预训练任务在无标签数据上进行训练。微调阶段中，BERT模型首先使用预训练参数进行初始化，然后使用下游任务中的标注数据对所有参数进行微调。

#### 架构

BERT的架构基于Transformer，是一种由多层双向Transformer编码器堆叠而成的深度神经网络。BERT的两个主要变体是BERT-base和BERT-large：

- **BERT-base**：12层Transformer编码器，隐藏层大小为768，注意力头数为12，总参数数约为110M。
- **BERT-large**：24层Transformer编码器，隐藏层大小为1024，注意力头数为16，总参数数约为340M。

## 神经网络

### **ppl**

PPL是用在自然语言处理领域（NLP）中，衡量语言模型好坏的指标。它主要是根据每个词来估计一句话出现的概率，并用句子长度作normalize。可以直观地理解为，在模型生成一句话时下一个词有90个合理选择，可选词数越少，我们大致认为模型越准确。这样也能解释，为什么**PPL越小，模型越好**。

### **激活函数**

激活函数的目的是解决实际问题中的非线性变换，线性变换只能拟合直线，而激活函数的加入，使神经网络具有了拟合曲线的能力。